
# pipeline.py (function-to-function, no CSV intermediate)
from __future__ import annotations
from typing import Dict, Any, Iterable, Tuple, List, Optional
import pandas as pd
import json
import os
import io
import gc
from psycopg import sql
from tqdm import tqdm
import torch

# Import your in-project preprocessors
from data.database_connector import DatabaseConnector
from data.query_helper import QueryHelper
from preprocess.numeric_preprocess import NumericPreprocessor
from preprocess.categorical_preprocess import CategoricalPreprocessor
from preprocess.text_preprocess import TextPreprocessor
from preprocess.upload_database import DataUploader

# Shared PK mapping (keep consistent with each module)
TABLE_PK_MAP = {
    'notice': ['bidntceno', 'bidntceord'],
    'company': ['bizno'],
}

TEXT_MODEL_NAME = os.getenv("TEXT_EMBEDDING_MODEL")

TEXT_COL = "bidntcenm"   # í…ìŠ¤íŠ¸ ì„ë² ë”© ëŒ€ìƒ ì»¬ëŸ¼
TEXT_EMB_DIM = 768       # ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ì°¨ì›



# ===== ì„¤ì • ==================================================================
def _read_json(path: str) -> Dict:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸ ì„¤ì • íŒŒì¼ ì—†ìŒ: {path} (ë¹ˆ ì„¤ì •ìœ¼ë¡œ ì§„í–‰)")
        return {}
    except Exception as e:
        print(f"âš ï¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {path} -> {e} (ë¹ˆ ì„¤ì •ìœ¼ë¡œ ì§„í–‰)")
        return {}
    

def load_preprocess_configs(table_name: str, meta_dir: str = "meta") -> Tuple[Dict, Dict, Dict]:
    """
    /meta/{table_name}_{type}_config.json ê·œì¹™ìœ¼ë¡œ ì„¤ì • ë¡œë“œ
    - type âˆˆ {numeric, categorical, text}
    - ì—†ìœ¼ë©´ {} ë°˜í™˜
    """
    t = table_name.strip().lower()
    files = {
        "numeric":     os.path.join(meta_dir, f"{t}_numeric_config.json"),
        "categorical": os.path.join(meta_dir, f"{t}_categorical_config.json"),
        "text":        os.path.join(meta_dir, f"{t}_text_config.json"),
    }
    num_cfg = _read_json(files["numeric"])
    cat_cfg = _read_json(files["categorical"])
    txt_cfg = _read_json(files["text"])
    return num_cfg, cat_cfg, txt_cfg


def _free():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def build_preprocessors(num_cfg, cat_cfg, txt_cfg, model_name: str | None = None):
    num_pp = NumericPreprocessor(config=num_cfg, table_pk_map=TABLE_PK_MAP)
    cat_pp = CategoricalPreprocessor(config=cat_cfg, table_pk_map=TABLE_PK_MAP)
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¸°ëŠ” í•œ ë²ˆë§Œ ë§Œë“¤ê³  ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì¬ì‚¬ìš© (ëª¨ë¸ ìºì‹œ ìœ ì§€)
    if model_name:
        txt_cfg = {k: {**v, "model_name": model_name} for k, v in txt_cfg.items()}
    txt_pp = TextPreprocessor(config=txt_cfg, table_pk_map=TABLE_PK_MAP)
    return num_pp, cat_pp, txt_pp


def fit_once_full(engine, qh: QueryHelper, table: str, num_pp, cat_pp):
    """ìˆ˜ì¹˜/ë²”ì£¼ fitì„ í•œ ë²ˆì—. í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì€ ì œì™¸í•´ì„œ ë©”ëª¨ë¦¬ ì ˆì•½."""
    sql = qh.get_use_columns_select(table, exclude_text_cols=[TEXT_COL])
    df = pd.read_sql_query(sql, engine)
    # PKëŠ” ë¬¸ìì—´ ë³´ì¡´
    for pk in TABLE_PK_MAP[table]:
        if pk in df.columns:
            df[pk] = df[pk].astype("string").fillna("")
    # fit (í…ìŠ¤íŠ¸ëŠ” fit í•„ìš” ì—†ìŒ)
    if hasattr(num_pp, "fit"): num_pp.fit(df)
    if hasattr(cat_pp, "fit"): cat_pp.fit(df)
    del df
    _free()

def _split_features(df: pd.DataFrame, pk: Iterable[str]) -> Tuple[List[str], List[str]]:
    """Return (feature_columns_without_null_flags, null_flag_columns)."""
    pk = list(pk or [])
    cols = list(map(str, df.columns))

    nulls = [c for c in cols if c.endswith("_is_null")]
    feats = [c for c in cols if c not in pk and not c.endswith("_is_null")]
    return feats, nulls


def transform_one_chunk(df_chunk: pd.DataFrame, table: str, num_pp, cat_pp, txt_pp) -> pd.DataFrame:
    """í•˜ë‚˜ì˜ ì²­í¬ë¥¼ num/cat/text ëª¨ë‘ transform í›„ PK ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•´ ë°˜í™˜."""
    pk = TABLE_PK_MAP[table]
    for c in pk:
        if c in df_chunk.columns:
            df_chunk[c] = df_chunk[c].astype("string").fillna("")
    # transformë§Œ í˜¸ì¶œ (fit ê¸ˆì§€)
    num_df = num_pp.transform(df_chunk, table) if num_pp else df_chunk[pk].copy()
    cat_df = cat_pp.transform(df_chunk, table) if cat_pp else df_chunk[pk].copy()
    txt_df = txt_pp.transform(df_chunk, table) if txt_pp else df_chunk[pk].copy()

    # merge
    merged = num_df.merge(cat_df, on=pk, how="left", validate="one_to_one")
    merged = merged.merge(txt_df, on=pk, how="left", validate="one_to_one")
    
    # ì»¬ëŸ¼ëª… ë¬¸ìì—´í™” ì•ˆì „ì¥ì¹˜
    merged.columns = merged.columns.map(str)
    return merged

def run_pipeline(table_name:str):
    db = DatabaseConnector()
    qh = QueryHelper(db)

    table = table_name  # í•„ìš” ì‹œ companyë„ ë™ì¼ ë¡œì§ ë³µì œ
    num_cfg, cat_cfg, txt_cfg = load_preprocess_configs(table)  # ë„ˆì˜ ê¸°ì¡´ ë¡œë” ì‚¬ìš©
    text_model = os.getenv("TEXT_EMBEDDING_MODEL")

    print("ğŸš€ ìˆ˜ì¹˜/ë²”ì£¼ fit 1íšŒ ìˆ˜í–‰")
    num_pp, cat_pp, txt_pp = build_preprocessors(num_cfg, cat_cfg, txt_cfg, model_name=text_model)
    fit_once_full(db.engine, qh, table, num_pp, cat_pp)

    print("ğŸš€ ì²­í¬ ë‹¨ìœ„ transform + ì—…ë¡œë“œ ì‹œì‘")
    # ëª¨ë“  ì‚¬ìš© ì»¬ëŸ¼(í…ìŠ¤íŠ¸ í¬í•¨) SELECT
    select_sql = qh.get_use_columns_select(table)  # í…ìŠ¤íŠ¸ ì œì™¸ X
    # ì§„í–‰ë¥  totalì´ í•„ìš”í•˜ë©´ count_rows ì‚¬ìš©
    # total = db.count_rows(qh.get_use_columns_count(table))

    uploader = DataUploader()
    uploader.if_exists = "replace"
    pk_cols = TABLE_PK_MAP[table]

    for i, chunk in enumerate(tqdm(db.iter_sql_chunks(select_sql, chunksize=50_000),
                                   desc=f"[XFORM/COPY] {table}")):
        out = transform_one_chunk(chunk, table, num_pp, cat_pp, txt_pp)
        uploader.upload_df(out, base_table_name=table, pk_cols=pk_cols)
        if i == 0:
            uploader.if_exists = "append"
        del chunk, out
        _free()

if __name__ == "__main__":
    run_pipeline("company")