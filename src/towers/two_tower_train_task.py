import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple, Union

from src.towers.two_tower_model import TwoTowerModel


class TwoTowerTrainTask(nn.Module):
    """
    Two-Tower í•™ìŠµ íƒœìŠ¤í¬ í´ë˜ìŠ¤
    - Positive pair ë°°ì¹˜ ì²˜ë¦¬
    - In-batch negative sampling
    - Cross entropy loss ê³„ì‚°
    """
    def __init__(
        self,
        two_tower_model: TwoTowerModel,
        temperature: float = 1.0,
        loss_type: str = "cross_entropy",
        label_smoothing: float = 0.0,
    ):
        """
        Args:
            two_tower_model: í•™ìŠµí•  TwoTowerModel ì¸ìŠ¤í„´ìŠ¤
            temperature: ìœ ì‚¬ë„ ê³„ì‚°ì‹œ temperature scaling
            loss_type: ì†ì‹¤ í•¨ìˆ˜ íƒ€ì… ("cross_entropy", "cosine_embedding")
            label_smoothing: Label smoothing ì •ë„ (0.0 ~ 1.0)
        """
        super().__init__()
        
        self.two_tower_model = two_tower_model
        self.temperature = temperature
        self.loss_type = loss_type
        self.label_smoothing = label_smoothing
        
        if loss_type not in ["cross_entropy", "cosine_embedding"]:
            raise ValueError(f"Unsupported loss_type: {loss_type}")
    
    def forward(
        self, 
        batch: Dict[str, Dict[str, torch.Tensor]],
        return_metrics: bool = False
    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            batch: {
                "notice": {"dense": Tensor, "kjt": KJT},
                "company": {"dense": Tensor, "kjt": KJT}
            }
            return_metrics: Trueë©´ ì¶”ê°€ ë©”íŠ¸ë¦­ë„ ë°˜í™˜
            
        Returns:
            return_metrics=False: loss tensor
            return_metrics=True: {"loss": tensor, "accuracy": tensor, "similarities": tensor}
        """
        notice_input = batch["notice"]
        company_input = batch["company"]
        
        # 1) ë°°ì¹˜ í¬ê¸° ê²€ì¦
        notice_batch_size = notice_input["dense"].size(0)
        company_batch_size = company_input["dense"].size(0)
        
        if notice_batch_size != company_batch_size:
            raise ValueError(
                f"Noticeì™€ Company ë°°ì¹˜ í¬ê¸°ê°€ ë‹¤ë¦…ë‹ˆë‹¤: {notice_batch_size} vs {company_batch_size}"
            )
        
        batch_size = notice_batch_size
        
        # 2) ì„ë² ë”© ê³„ì‚°
        notice_embeddings, company_embeddings = self.two_tower_model(
            notice_input, company_input
        )
        
        # 3) ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° (In-batch negative sampling)
        similarity_matrix = self._compute_similarity_matrix(
            notice_embeddings, company_embeddings
        )  # [B, B]
        
        # 4) Positive pair ì •í•©ì„± ê²€ì¦ (ìµœì´ˆ 1íšŒë§Œ)
        if not hasattr(self, '_pair_check_done'):
            self._verify_positive_pair_alignment(similarity_matrix)
            self._pair_check_done = True

        # 5) ì†ì‹¤ ê³„ì‚°
        loss = self._compute_loss(similarity_matrix, batch_size)
        
        if return_metrics:
            metrics = self._compute_metrics(similarity_matrix, batch_size)
            return {
                "loss": loss,
                **metrics,
                "similarity_matrix": similarity_matrix.detach()
            }
        else:
            return loss
    
    def _compute_similarity_matrix(
        self, 
        notice_embeddings: torch.Tensor, 
        company_embeddings: torch.Tensor
    ) -> torch.Tensor:
        """ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°"""
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (L2 ì •ê·œí™”ëœ ë²¡í„°ì´ë¯€ë¡œ)
        similarity_matrix = torch.mm(notice_embeddings, company_embeddings.t())
        
        # Temperature scaling ì ìš©
        if self.temperature != 1.0:
            similarity_matrix = similarity_matrix / self.temperature
            
        return similarity_matrix
    
    def _compute_loss(self, similarity_matrix: torch.Tensor, batch_size: int) -> torch.Tensor:
        """ì†ì‹¤ ê³„ì‚°"""
        # similarity_matrix[j, i] = sÎ¸(uj, vi)
        if self.loss_type == "cross_entropy":
            # ëŒ€ê°ì„ ì´ positive pair (ì •ë‹µ ë ˆì´ë¸”)
            labels = torch.arange(batch_size, device=similarity_matrix.device)

            # ì–‘ë°©í–¥ Cross Entropy Loss (ë” ê°•í•œ í•™ìŠµ ì‹ í˜¸)
            loss_notice_to_company = F.cross_entropy(
                similarity_matrix,
                labels,
                label_smoothing=self.label_smoothing
            )
            loss_company_to_notice = F.cross_entropy(
                similarity_matrix.t(),
                labels,
                label_smoothing=self.label_smoothing
            )

            # í‰ê· ìœ¼ë¡œ ê²°í•©
            loss = 0.5 * (loss_notice_to_company + loss_company_to_notice)
            
        elif self.loss_type == "cosine_embedding":
            # Cosine embedding loss êµ¬í˜„
            batch_size = similarity_matrix.size(0)
            device = similarity_matrix.device
            
            # Positive pairs (ëŒ€ê°ì„ )
            positive_similarities = torch.diag(similarity_matrix)
            positive_targets = torch.ones(batch_size, device=device)
            
            # Negative pairs (ë¹„ëŒ€ê°ì„ )
            mask = ~torch.eye(batch_size, dtype=torch.bool, device=device)
            negative_similarities = similarity_matrix[mask]
            negative_targets = -torch.ones(len(negative_similarities), device=device)
            
            # ì „ì²´ ì†ì‹¤
            all_similarities = torch.cat([positive_similarities, negative_similarities])
            all_targets = torch.cat([positive_targets, negative_targets])
            
            loss = F.cosine_embedding_loss(
                all_similarities.unsqueeze(1), 
                torch.ones_like(all_similarities.unsqueeze(1)), 
                all_targets
            )
        
        return loss
    
    def _compute_metrics(self, similarity_matrix: torch.Tensor, batch_size: int) -> Dict[str, torch.Tensor]:
        """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # Top-1 ì •í™•ë„ (ê° ê³µê³ ê°€ ìì‹ ì˜ ì—…ì²´ë¥¼ 1ìˆœìœ„ë¡œ ì„ íƒí•˜ëŠ” ë¹„ìœ¨)
        predicted_indices = torch.argmax(similarity_matrix, dim=1)
        correct_indices = torch.arange(batch_size, device=similarity_matrix.device)
        accuracy = (predicted_indices == correct_indices).float().mean()
        
        # ëŒ€ê°ì„  vs í‰ê·  ìœ ì‚¬ë„
        diagonal_similarities = torch.diag(similarity_matrix)
        off_diagonal_mask = ~torch.eye(batch_size, dtype=torch.bool, device=similarity_matrix.device)
        off_diagonal_similarities = similarity_matrix[off_diagonal_mask]
        
        return {
            "accuracy": accuracy,
            "positive_similarity_mean": diagonal_similarities.mean(),
            "negative_similarity_mean": off_diagonal_similarities.mean(),
            "similarity_gap": diagonal_similarities.mean() - off_diagonal_similarities.mean()
        }
    
    def predict_batch(
        self, 
        batch: Dict[str, Dict[str, torch.Tensor]], 
        top_k: int = 10
    ) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ì— ëŒ€í•œ Top-K ì˜ˆì¸¡ (ì¶”ë¡ ìš©)"""
        self.eval()
        with torch.no_grad():
            notice_input = batch["notice"]
            company_input = batch["company"]
            
            notice_embeddings, company_embeddings = self.two_tower_model(
                notice_input, company_input
            )
            
            similarity_matrix = self._compute_similarity_matrix(
                notice_embeddings, company_embeddings
            )
            
            # Top-K ì„ íƒ
            top_similarities, top_indices = torch.topk(similarity_matrix, k=top_k, dim=1)
            
            return {
                "top_similarities": top_similarities,  # [B, K]
                "top_indices": top_indices,            # [B, K]
                "all_similarities": similarity_matrix  # [B, B]
            }


# Helper í•¨ìˆ˜ë“¤
def create_two_tower_train_task(
    notice_categorical_keys,
    company_categorical_keys,
    metadata_path: str = "meta/metadata.csv",
    categorical_embedding_dim: int = 64,
    notice_dense_input_dim: int = 256,
    company_dense_input_dim: int = 128,
    tower_hidden_dims = None,
    final_embedding_dim: int = 128,
    dropout_rate: float = 0.2,
    temperature: float = 1.0,
    loss_type: str = "cross_entropy",
    device = "cuda:0"
) -> TwoTowerTrainTask:
    """TwoTowerTrainTask ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    from src.towers.two_tower_model import create_two_tower_model
    
    # TwoTowerModel ìƒì„±
    two_tower_model = create_two_tower_model(
        notice_categorical_keys=notice_categorical_keys,
        company_categorical_keys=company_categorical_keys,
        metadata_path=metadata_path,
        categorical_embedding_dim=categorical_embedding_dim,
        notice_dense_input_dim=notice_dense_input_dim,
        company_dense_input_dim=company_dense_input_dim,
        tower_hidden_dims=tower_hidden_dims,
        final_embedding_dim=final_embedding_dim,
        dropout_rate=dropout_rate,
        device=device
    )
    
    # TrainTask ìƒì„±
    train_task = TwoTowerTrainTask(
        two_tower_model=two_tower_model,
        temperature=temperature,
        loss_type=loss_type
    )
    
    return train_task


# ì¶”ê°€ ë©”ì„œë“œë“¤ì„ TwoTowerTrainTask í´ë˜ìŠ¤ì— ë™ì ìœ¼ë¡œ ì¶”ê°€
def _verify_positive_pair_alignment(self, similarity_matrix: torch.Tensor):
    """Positive pair ì •í•©ì„± ê²€ì¦ - ëŒ€ê°ì„ ì´ ì§„ì§œ ì •ë‹µì¸ì§€ í™•ì¸"""
    B = similarity_matrix.size(0)

    # ê° noticeê°€ ê°€ì¥ ìœ ì‚¬í•œ company ì„ íƒ (row-wise)
    row_top1 = similarity_matrix.argmax(dim=1)
    # ê° companyê°€ ê°€ì¥ ìœ ì‚¬í•œ notice ì„ íƒ (col-wise)
    col_top1 = similarity_matrix.argmax(dim=0)

    # ëŒ€ê°ì„  ì¸ë±ìŠ¤ (ì •ë‹µì´ì–´ì•¼ í•  ìœ„ì¹˜)
    diag_idx = torch.arange(B, device=similarity_matrix.device)

    # ì •í™•ë„ ê³„ì‚°
    row_hit = (row_top1 == diag_idx).float().mean().item()  # notice â†’ company
    col_hit = (col_top1 == diag_idx).float().mean().item()  # company â†’ notice

    print(f"ğŸ” [Positive Pair Alignment Check]")
    print(f"   ğŸ“Š Noticeâ†’Company Top-1 ì •í™•ë„: {row_hit:.3f}")
    print(f"   ğŸ“Š Companyâ†’Notice Top-1 ì •í™•ë„: {col_hit:.3f}")

    # ëŒ€ê°ì„  vs ìµœëŒ€ê°’ ë¶„ì„
    diag_values = similarity_matrix.diag()
    max_values = similarity_matrix.max(dim=1)[0]
    diag_is_max = (diag_values == max_values).float().mean().item()

    print(f"   ğŸ¯ ëŒ€ê°ì„ ì´ row-maxì¸ ë¹„ìœ¨: {diag_is_max:.3f}")

    # ê²½ê³  íŒì •
    if row_hit < 0.05 and col_hit < 0.05:
        print("   ğŸš¨ CRITICAL: Positive pair ì •í•©ì„± ì‹¤íŒ¨!")
        print("   â†’ DataLoaderì—ì„œ notice/company ìˆœì„œê°€ ì–´ê¸‹ë‚¨")
        print("   â†’ ë™ì¼í•œ ìƒ˜í”ŒëŸ¬/ì¸ë±ìŠ¤ë¡œ í˜ì–´ë¥¼ êµ¬ì„±í•˜ì„¸ìš”")
    elif row_hit < 0.3 or col_hit < 0.3:
        print("   âš ï¸  WARNING: Positive pair ì •í•©ì„± ë¶€ì¡±")
        print("   â†’ ë°°ì¹˜ ë‚´ positive ë¹„ìœ¨ í™•ì¸ í•„ìš”")
    else:
        print("   âœ… Positive pair ì •í•©ì„± ì–‘í˜¸")
    print()

# ë©”ì„œë“œë¥¼ í´ë˜ìŠ¤ì— ë°”ì¸ë”©
TwoTowerTrainTask._verify_positive_pair_alignment = _verify_positive_pair_alignment