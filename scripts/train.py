#!/usr/bin/env python3
"""
Simple Two-Tower Model Training Script
ê°„ë‹¨í•œ í•™ìŠµ ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸
"""

import torch
import torch.optim as optim
from tqdm import tqdm
import time
from pathlib import Path
import pandas as pd
from datetime import datetime
import os

# Project imports
from data.database_connector import DatabaseConnector
from src.torchrec_preprocess.schema import build_torchrec_schema_from_meta
from src.towers.pairs.unified_bid_data_loader import create_unified_bid_dataloaders
from src.towers.two_tower_train_task import create_two_tower_train_task
from src.evaluation.evaluator import TwoTowerEvaluator


def save_training_results(hyperparams, metrics, output_file="train_results.csv"):
    """
    í•™ìŠµ ê²°ê³¼ë¥¼ CSV íŒŒì¼ì— ê¸°ë¡

    Args:
        hyperparams: í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        metrics: ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        output_file: ì¶œë ¥ CSV íŒŒì¼ëª…
    """
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
    result_row = {
        "timestamp": timestamp,
        "batch_size": hyperparams.get("batch_size", "N/A"),
        "model_params": hyperparams.get("model_params", "N/A"),
        "embedding_dim": hyperparams.get("embedding_dim", "N/A"),
        "final_embedding_dim": hyperparams.get("final_embedding_dim", "N/A"),
        "hidden_dims": str(hyperparams.get("hidden_dims", "N/A")),
        "learning_rate": hyperparams.get("learning_rate", "N/A"),
        "epochs": hyperparams.get("epochs", "N/A"),
        "train_loss": metrics.get("train_loss", "N/A"),
        "train_acc": metrics.get("train_acc", "N/A"),
        "val_loss": metrics.get("val_loss", "N/A"),
        "val_acc": metrics.get("val_acc", "N/A"),
        "recall_at_5": metrics.get("recall_at_5", "N/A"),
        "recall_at_10": metrics.get("recall_at_10", "N/A"),
        "mrr": metrics.get("mrr", "N/A"),
        "similarity_gap": metrics.get("similarity_gap", "N/A"),
        "train_batches": hyperparams.get("train_batches", "N/A"),
        "test_batches": hyperparams.get("test_batches", "N/A"),
        "gpu_optimization": hyperparams.get("gpu_optimization", "N/A"),
    }

    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    new_row_df = pd.DataFrame([result_row])

    # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
    if os.path.exists(output_file):
        # ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€
        existing_df = pd.read_csv(output_file)
        updated_df = pd.concat([existing_df, new_row_df], ignore_index=True)
        print(f"ê¸°ì¡´ ê²°ê³¼ì— ìƒˆ í–‰ ì¶”ê°€: {output_file}")
    else:
        # ìƒˆ íŒŒì¼ ìƒì„±
        updated_df = new_row_df
        print(f"ìƒˆ ê²°ê³¼ íŒŒì¼ ìƒì„±: {output_file}")

    # CSV íŒŒì¼ ì €ì¥
    updated_df.to_csv(output_file, index=False)
    print(f"í•™ìŠµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(updated_df)}í–‰")


def main():
    print("=== Two-Tower ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")

    # ===========================================
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ì¤‘ì•™ ê´€ë¦¬)
    # ===========================================
    config = {
        # ë°ì´í„° ì„¤ì •
        "batch_size": 256,
        "test_split": 0.2,
        "shuffle_seed": 42,
        "pair_limit": 1000000,

        # DataLoader ì„¤ì •
        "num_workers": 0,
        "pin_memory": False,
        "streaming": False,
        "load_all_features": True,
        "chunk_size": 1000000,
        "feature_chunksize": 1000,
        "use_preprocessor": True,
        "test_mode": True,          # ë¹ ì§„ ì¤‘ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°!
        "prefetch_factor": 2,

        # ëª¨ë¸ ì•„í‚¤í…ì²˜
        "categorical_embedding_dim": 32,
        "notice_dense_input_dim": 256,
        "company_dense_input_dim": 128,
        "tower_hidden_dims": [512, 256],
        "final_embedding_dim": 128,
        "dropout_rate": 0.1,
        "temperature": 1.0,
        "loss_type": "cross_entropy",
        "label_smoothing": 0.0,

        # í•™ìŠµ ì„¤ì •
        "learning_rate": 1e-3,
        "weight_decay": 1e-5,
        "num_epochs": 1,
        "warmup_ratio": 0.05,
        "log_interval": 20,

        # ëª¨ë¸ ì €ì¥/ë¡œë”©
        "output_dir": "output/models",
        "save_best": True,
        "save_final": True,

        # CUDA ìµœì í™”
        "enable_tf32": True,
        "enable_cudnn_benchmark": True,
        "enable_torch_compile": False,  # í˜„ì¬ ì£¼ì„ì²˜ë¦¬ë¨
        "compile_mode": "reduce-overhead",

        # ì‹œìŠ¤í…œ ì„¤ì •
        "gpu_optimization": "GPU Collate + CUDA Streams + TF32/cuDNN",
        "metadata_path": "meta/metadata.csv"
    }

    print(f"ğŸ”§ ì„¤ì •ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(f"   - Test Mode: {config['test_mode']} (Pair Limit: {config['pair_limit']:,})")
    print(f"   - Batch Size: {config['batch_size']}")
    print(f"   - Embedding Dim: {config['categorical_embedding_dim']} â†’ {config['final_embedding_dim']}")
    print(f"   - Hidden Dims: {config['tower_hidden_dims']}")
    print(f"   - Learning Rate: {config['learning_rate']}")
    print(f"   - GPU Optimization: {config['enable_torch_compile']}")
    print(f"   - Temperature: {config['temperature']}")

    # CUDA ê°€ì† ìµœì í™” (config ê¸°ë°˜)
    if config["enable_tf32"]:
        torch.backends.cuda.matmul.allow_tf32 = True         # TF32 í—ˆìš© (Ampere+)
        torch.set_float32_matmul_precision("high")           # cublas ê³ ì •ë°€ ì†ë„ ëª¨ë“œ
    if config["enable_cudnn_benchmark"]:
        torch.backends.cudnn.benchmark = True                # Conv/Norm autotune (MLPì—ë„ ë„ì›€)

    # 1. ê¸°ë³¸ ì„¤ì •
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print("CUDA ê°€ì† ìµœì í™” í™œì„±í™”: TF32, cuDNN benchmark, high precision matmul")
    
    # 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    print("\në°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
    db = DatabaseConnector()
    engine = db.engine
    
    # 3. ìŠ¤í‚¤ë§ˆ êµ¬ì¶•
    print("ìŠ¤í‚¤ë§ˆ êµ¬ì¶• ì¤‘...")
    schema_config = {
        "notice_table": "notice",
        "company_table": "company",
        "pair_table": "bid_two_tower",
        "pair_notice_id_cols": ["bidntceno", "bidntceord"],
        "pair_company_id_cols": ["bizno"],
        "metadata_path": "meta/metadata.csv"
    }
    schema = build_torchrec_schema_from_meta(**schema_config)
    
    print(f"Notice í”¼ì²˜: {len(schema.notice.categorical)}ê°œ ë²”ì£¼í˜•, {len(schema.notice.numeric)}ê°œ ìˆ˜ì¹˜í˜•")
    print(f"Company í”¼ì²˜: {len(schema.company.categorical)}ê°œ ë²”ì£¼í˜•, {len(schema.company.numeric)}ê°œ ìˆ˜ì¹˜í˜•")
    
    # 4. ë°ì´í„°ë¡œë” ìƒì„± (Test Mode + GPU ìµœì í™”)
    print("\në°ì´í„°ë¡œë” ìƒì„± ì¤‘... (Test Mode + GPU Collate ìµœì í™”)")
    train_loader, test_loader = create_unified_bid_dataloaders(
        db_engine=engine,
        schema=schema,
        batch_size=config["batch_size"],
        test_split=config["test_split"],
        shuffle_seed=config["shuffle_seed"],
        num_workers=config["num_workers"],
        pin_memory=config["pin_memory"],
        streaming=config["streaming"],
        load_all_features=config["load_all_features"],
        chunk_size=config["chunk_size"],
        feature_chunksize=config["feature_chunksize"],
        use_preprocessor=config["use_preprocessor"],
        test_mode=config["test_mode"],
        pair_limit=config["pair_limit"],
    )
    
    print(f"Train ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
    print(f"Test ë°°ì¹˜ ìˆ˜: {len(test_loader) if test_loader else 'None'}")
    
    # 5. ëª¨ë¸ ìƒì„±
    print("\nëª¨ë¸ ìƒì„± ì¤‘...")
    
    # ë²”ì£¼í˜• í‚¤ ì¶”ì¶œ
    notice_categorical_keys = schema.notice.categorical
    company_categorical_keys = schema.company.categorical
    
    # TrainTask ìƒì„± (config ì‚¬ìš©)
    train_task = create_two_tower_train_task(
        notice_categorical_keys=notice_categorical_keys,
        company_categorical_keys=company_categorical_keys,
        metadata_path=config["metadata_path"],
        categorical_embedding_dim=config["categorical_embedding_dim"],
        notice_dense_input_dim=config["notice_dense_input_dim"],
        company_dense_input_dim=config["company_dense_input_dim"],
        tower_hidden_dims=config["tower_hidden_dims"],
        final_embedding_dim=config["final_embedding_dim"],
        dropout_rate=config["dropout_rate"],
        temperature=config["temperature"],
        loss_type=config["loss_type"],
        device=device
    )

    # torch.compile ìµœì í™” (config ê¸°ë°˜)
    if config["enable_torch_compile"]:
        print("torch.compile ìµœì í™” ì ìš© ì¤‘...")
        train_task = torch.compile(train_task, mode=config["compile_mode"], fullgraph=False)
        print(f"torch.compile ì ìš© ì™„ë£Œ ({config['compile_mode']} ëª¨ë“œ)")
    else:
        print("torch.compile ë¹„í™œì„±í™”ë¨")
    
    # 6. ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (config ì‚¬ìš©)
    optimizer = optim.Adam(train_task.parameters(), lr=config["learning_rate"], weight_decay=config["weight_decay"])

    # Learning Rate Warmup ìŠ¤ì¼€ì¤„ëŸ¬ (config warmup_ratio ì‚¬ìš©)
    from torch.optim.lr_scheduler import LambdaLR
    warmup_steps = max(1, int(len(train_loader) * config["warmup_ratio"]))

    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        return 1.0

    scheduler = LambdaLR(optimizer, lr_lambda, last_epoch=-1)
    
    # 7. ëª¨ë¸ ì •ë³´ ì¶œë ¥
    total_params = sum(p.numel() for p in train_task.parameters())
    trainable_params = sum(p.numel() for p in train_task.parameters() if p.requires_grad)
    print(f"\nëª¨ë¸ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ (í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}ê°œ)")
    print(f"Learning Rate Warmup: {warmup_steps} steps ({warmup_steps/len(train_loader)*100:.1f}% of epoch)")
    
    evaluator = TwoTowerEvaluator(device=device)
    
    # 8. GPU ìµœì í™” ì„¤ì •
    print("\n=== GPU ìµœì í™” ëª¨ë“œ (GPU Collate + ë¹„ë™ê¸° ì „ì†¡) ===")
    print(f"  - GPU Collate: KJTë¥¼ GPUì—ì„œ ì§ì ‘ ìƒì„±")
    print(f"  - Pin memory: False (GPU ì§ì ‘ ìƒì„±)")
    print(f"  - ë¹„ë™ê¸° H2D ì „ì†¡ ì¤€ë¹„")

    # CUDA ìŠ¤íŠ¸ë¦¼ ì„¤ì • (ë¹„ë™ê¸° ì „ì†¡ìš©)
    prefetch_stream = torch.cuda.Stream()

    def _to_device_async(batch, device):
        """ë¹„ë™ê¸° GPU ì „ì†¡ (í•„ìš”í•œ ê²½ìš°ë§Œ)"""
        # ì´ë¯¸ GPUë¼ë©´ ë°”ë¡œ ë¦¬í„´
        if batch["notice"]["dense"].is_cuda and batch["company"]["dense"].is_cuda:
            return batch
        with torch.cuda.stream(prefetch_stream):
            batch["notice"]["dense"] = batch["notice"]["dense"].to(device, non_blocking=True)
            batch["company"]["dense"] = batch["company"]["dense"].to(device, non_blocking=True)
            if hasattr(batch["notice"]["kjt"], "to"):
                batch["notice"]["kjt"] = batch["notice"]["kjt"].to(device)
            if hasattr(batch["company"]["kjt"], "to"):
                batch["company"]["kjt"] = batch["company"]["kjt"].to(device)
        return batch

    # 9. í•™ìŠµ ë£¨í”„ (config ì‚¬ìš©)
    print("\n=== í•™ìŠµ ì‹œì‘ (GPU ìµœì í™”) ===")
    num_epochs = config["num_epochs"]
    best_val_loss = float('inf')
    output_dir = Path(config["output_dir"])

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        
        # í•™ìŠµ
        train_task.train()
        train_losses = []
        train_accuracies = []
        
        # ì´ì¤‘ ë¼ì¸ tqdm ì„¤ì • (ìœ—ì¤„: ì§„í–‰ë°”, ì•„ë«ì¤„: ìƒì„¸ ì •ë³´)
        train_pbar = tqdm(
            train_loader,
            desc="Training",
            unit="batch",
            position=0,  # 0ë²ˆì§¸ ì¤„ (ìœ—ì¤„)
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {rate_fmt}',
            ncols=100,
            mininterval=0.5,
            leave=False  # ì™„ë£Œ í›„ ì§„í–‰ë°” ì œê±°
        )

        # ìƒì„¸ ì •ë³´ìš© tqdm (ì•„ë«ì¤„)
        info_bar = tqdm(
            total=0,
            position=1,  # 1ë²ˆì§¸ ì¤„ (ì•„ë«ì¤„)
            bar_format='{desc}',
            ncols=100,
            leave=False
        )
        
        step_count = 0
        log_interval = config["log_interval"]  # configì—ì„œ ë¡œê·¸ ê°„ê²© ì„¤ì •

        for batch in train_pbar:
            # ë¹„ë™ê¸° GPU ì „ì†¡ (í•„ìš”í•œ ê²½ìš°ë§Œ)
            batch = _to_device_async(batch, device)
            torch.cuda.current_stream().wait_stream(prefetch_stream)  # ì•ˆì „ ë™ê¸°í™”

            # Forward pass
            optimizer.zero_grad()
            result = train_task(batch, return_metrics=True)

            loss = result["loss"]
            accuracy = result["accuracy"]

            # Backward pass
            loss.backward()
            optimizer.step()

            # Scheduler step (optimizer.step() ì´í›„ì— í˜¸ì¶œ)
            scheduler.step()

            step_count += 1

            # ë©”íŠ¸ë¦­ ì €ì¥ (ë§¤ ìŠ¤í…)
            train_losses.append(loss.item())
            train_accuracies.append(accuracy.item())

            # Progress bar ì—…ë°ì´íŠ¸ (ì£¼ê¸°ì ìœ¼ë¡œë§Œ - GPU ë™ê¸°í™” ê°ì†Œ)
            if step_count % log_interval == 0:
                # Two-Tower í•µì‹¬ ì§€í‘œ 5ê°œ ì¶”ì¶œ
                loss_val = loss.item()
                accuracy_val = accuracy.item()
                pos_sim = result.get("positive_similarity_mean", torch.tensor(0.0)).item()
                neg_sim = result.get("negative_similarity_mean", torch.tensor(0.0)).item()
                sim_gap = result.get("similarity_gap", torch.tensor(0.0)).item()

                # Z-gap ê³„ì‚° (similarity_gapì„ í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”í•œ ì§€í‘œ)
                z_gap = sim_gap / max(abs(neg_sim) + 1e-8, 1e-8)  # 0 division ë°©ì§€

                # ì•„ë«ì¤„ì— Two-Tower í•µì‹¬ ì§€í‘œ í‘œì‹œ
                info_str = f"ğŸ“Š Loss: {loss_val:.3f} | Acc: {accuracy_val:.3f} | Pos: {pos_sim:.3f} | Neg: {neg_sim:.3f} | Z-gap: {z_gap:.2f} | Batch: {config['batch_size']}"
                info_bar.set_description_str(info_str)
            
        
        # ì§„í–‰ë°” ì¢…ë£Œ
        train_pbar.close()
        info_bar.close()

        # ì—í¬í¬ ê²°ê³¼
        avg_train_loss = sum(train_losses) / len(train_losses)
        avg_train_acc = sum(train_accuracies) / len(train_accuracies)

        print(f"Train - Loss: {avg_train_loss:.4f}, Accuracy: {avg_train_acc:.3f}")
        
        # ê²€ì¦ (ìˆë‹¤ë©´)
        avg_val_loss = avg_train_loss  # ê¸°ë³¸ê°’
        if test_loader is not None:
            train_task.eval()
            val_losses = []
            val_accuracies = []
            
            with torch.no_grad():
                # Validationìš© ì´ì¤‘ ë¼ì¸ ì„¤ì •
                val_pbar = tqdm(
                    test_loader,
                    desc="Validation",
                    unit="batch",
                    position=0,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {rate_fmt}',
                    ncols=100,
                    mininterval=0.5,
                    leave=False
                )

                val_info_bar = tqdm(
                    total=0,
                    position=1,
                    bar_format='{desc}',
                    ncols=100,
                    leave=False
                )

                for batch in val_pbar:
                    result = train_task(batch, return_metrics=True)
                    
                    loss = result["loss"]
                    accuracy = result["accuracy"]
                    
                    val_losses.append(loss.item())
                    val_accuracies.append(accuracy.item())

                    # Validation Two-Tower í•µì‹¬ ì§€í‘œ 5ê°œ
                    val_loss = loss.item()
                    val_acc = accuracy.item()
                    val_pos_sim = result.get("positive_similarity_mean", torch.tensor(0.0)).item()
                    val_neg_sim = result.get("negative_similarity_mean", torch.tensor(0.0)).item()
                    val_sim_gap = result.get("similarity_gap", torch.tensor(0.0)).item()

                    # Validation Z-gap ê³„ì‚°
                    val_z_gap = val_sim_gap / max(abs(val_neg_sim) + 1e-8, 1e-8)

                    # Validation ì •ë³´ ì•„ë«ì¤„ì— í‘œì‹œ
                    info_str = f"ğŸ” Loss: {val_loss:.3f} | Acc: {val_acc:.3f} | Pos: {val_pos_sim:.3f} | Neg: {val_neg_sim:.3f} | Z-gap: {val_z_gap:.2f}"
                    val_info_bar.set_description_str(info_str)

                # Validation ì§„í–‰ë°” ì¢…ë£Œ
                val_pbar.close()
                val_info_bar.close()
            
            avg_val_loss = sum(val_losses) / len(val_losses)
            avg_val_acc = sum(val_accuracies) / len(val_accuracies)
            
            print(f"Val   - Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_acc:.3f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (config ê¸°ë°˜)
        save_checkpoint(train_task, optimizer, epoch, avg_val_loss, output_dir)

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if config["save_best"] and avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            save_checkpoint(train_task, optimizer, epoch, avg_val_loss, output_dir, is_best=True)
            print(f"ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! Loss: {avg_val_loss:.4f}")
            
            
    # í•™ìŠµ ì™„ë£Œ
    print("\n=== í•™ìŠµ ì™„ë£Œ ===")
    print("ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ í•™ìŠµ ì™„ë£Œ!")
            
    print("\n=== ìµœì¢… í‰ê°€ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸ ===")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¢…í•© í‰ê°€
    if test_loader is not None:
        print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¢…í•© í‰ê°€:")
        test_metrics = evaluator.evaluate_comprehensive(train_task, test_loader, verbose=True)
    else:
        print("í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œë¡œ í‰ê°€:")
        sample_batch = next(iter(train_loader))
        sample_metrics = evaluator.evaluate_single_batch(train_task, sample_batch, verbose=True)
    
    # ì¶”ë¡  ì‹œì—°
    test_batch = next(iter(train_loader))
    evaluator.demonstrate_predictions(train_task, test_batch, top_k=10)

    # í•™ìŠµ ê²°ê³¼ CSV ì €ì¥
    print("\n=== í•™ìŠµ ê²°ê³¼ ê¸°ë¡ ===")

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ êµ¬ì„± (config ê¸°ë°˜)
    hyperparams = {
        "batch_size": config["batch_size"],
        "model_params": total_params,
        "embedding_dim": config["categorical_embedding_dim"],
        "final_embedding_dim": config["final_embedding_dim"],
        "hidden_dims": config["tower_hidden_dims"],
        "learning_rate": config["learning_rate"],
        "weight_decay": config["weight_decay"],
        "dropout_rate": config["dropout_rate"],
        "temperature": config["temperature"],
        "epochs": config["num_epochs"],
        "train_batches": len(train_loader),
        "test_batches": len(test_loader) if test_loader else 0,
        "gpu_optimization": config["gpu_optimization"]
    }

    # ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬ êµ¬ì„± (ìµœì¢… í‰ê°€ ê²°ê³¼ ì‚¬ìš©)
    final_metrics = {
        "train_loss": avg_train_loss,
        "train_acc": avg_train_acc,
        "val_loss": avg_val_loss if test_loader else "N/A",
        "val_acc": avg_val_acc if test_loader else "N/A",
        "recall@5": test_metrics.get("recall@5", "N/A") if test_loader else "N/A",
        "recall@10": test_metrics.get("recall@10", "N/A") if test_loader else "N/A",
        "mrr": test_metrics.get("mrr", "N/A") if test_loader else "N/A",
        "similarity_gap": test_metrics.get("similarity_gap", "N/A") if test_loader else "N/A"
    }

    # CSV ì €ì¥
    save_training_results(hyperparams, final_metrics)

    # 9. ìµœì¢… ëª¨ë¸ ì €ì¥ (config ê¸°ë°˜)
    if config["save_final"]:
        save_checkpoint(train_task, optimizer, num_epochs-1, 0.0, output_dir, is_final=True)
        print(f"\nìµœì¢… ëª¨ë¸ ì €ì¥: {output_dir}")

    print("\n=== í•™ìŠµ ì™„ë£Œ ===")


def save_checkpoint(model, optimizer, epoch, loss, save_dir, is_best=False, is_final=False):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    import os
    from pathlib import Path
    
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }
    
    # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    if not is_final:
        checkpoint_path = save_dir / f'checkpoint_epoch_{epoch+1}.pt'
        torch.save(checkpoint, checkpoint_path)
        print(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if is_best:
        best_path = save_dir / 'best_model.pt'
        torch.save(checkpoint, best_path)
        print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥ (inferenceìš©)
    if is_final:
        final_path = save_dir / 'final_model.pt'
        torch.save(checkpoint, final_path)
        
        # ëª¨ë¸ë§Œ ë”°ë¡œ ì €ì¥ (ì¶”ë¡ ìš©)
        model_only_path = save_dir / 'model_weights.pt'
        torch.save(model.state_dict(), model_only_path)
        print(f"ìµœì¢… ëª¨ë¸ ì €ì¥: {final_path}")
        print(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥: {model_only_path}")


def load_checkpoint(model, optimizer, checkpoint_path):
    """ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
    print(f"ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location='cuda:0')
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    last_loss = checkpoint['loss']
    
    print(f"Epoch {checkpoint['epoch']+1}ë¶€í„° ì´ì–´ì„œ í•™ìŠµ ì‹œì‘")
    print(f"ì´ì „ ì†ì‹¤: {last_loss:.4f}")
    
    return start_epoch, last_loss


def resume_training_example():
    """ì´ì–´í•™ìŠµ ì˜ˆì œ"""
    print("=== ì´ì–´í•™ìŠµ ëª¨ë“œ ===")
    
    # ê¸°ë³¸ ì„¤ì •ì€ main()ê³¼ ë™ì¼...
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    
    # ë°ì´í„°ë¡œë” ìƒì„± (main()ê³¼ ë™ì¼)
    # ... ìƒëµ ...
    
    # ëª¨ë¸ ìƒì„± (main()ê³¼ ë™ì¼) 
    # ... ìƒëµ ...
    
    # ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    checkpoint_path = "output/checkpoint_epoch_3.pt"  # ì˜ˆì‹œ
    if Path(checkpoint_path).exists():
        start_epoch, last_loss = load_checkpoint(train_task, optimizer, checkpoint_path)
    else:
        print(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        return
    
    # ë‚¨ì€ ì—í¬í¬ í•™ìŠµ
    num_epochs = 10  # ì „ì²´ ëª©í‘œ ì—í¬í¬
    for epoch in range(start_epoch, num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs} (ì´ì–´í•™ìŠµ)")
        # ... í•™ìŠµ ì½”ë“œ ...


if __name__ == "__main__":
    import sys
    
    # ì´ì–´í•™ìŠµ ëª¨ë“œì¸ì§€ í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "--resume":
        resume_training_example()
    else:
        main()